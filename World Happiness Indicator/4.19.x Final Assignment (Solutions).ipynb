{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fced9a7c-bf3f-478c-9860-5d24a88b172b",
   "metadata": {},
   "source": [
    "# 4.19.x Final Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a815f-7e3e-4032-9721-eb5409fbc523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "sns.set(rc={'figure.figsize':(15, 9)})\n",
    "sns.set(font_scale=1.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d5563-e2df-4934-b313-c15a3fc6e6d2",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "The `food_consumption.csv` dataset from the `food` folder contains data on the several countries' food consumption per food category and their respective CO2 emissions. Load it to a DataFrame named `food` and check its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd1629-235c-4764-9637-447f32323a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "food = pd.read_csv('data/food_consumption.csv')\n",
    "food.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b304f-f6f4-4a5b-a662-71e0250b2cce",
   "metadata": {},
   "source": [
    "1. Use the `.describe()` method on the `food` DataFrame to produce descriptive statistics about the `consumption` metric for each class in the `food_category` variable. **Which `food_category` has the highest median value of `food_consumption`?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f2aae-2396-4fa5-a6bd-1f7080963bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "food.groupby('food_category')['consumption'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3112d1-8b48-4634-a091-d07c04f9f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cat_cons = food.groupby('food_category')['consumption'].describe().sort_values('50%', ascending=False).head(1).index[0]\n",
    "print(f'The food category with the highest median value of consumption is {max_cat_cons}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78301ce2-ff86-4065-b681-3f08468dca5a",
   "metadata": {},
   "source": [
    "2. In a single chart, plot one boxplot for each `food_category` (11 in total) using the variable `co2_emission` as the metric. **By looking at the chart, which `food_category` has the highest interquartile range (IQR)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f44cd1c-f07e-47c8-ae66-bb88bbfa1216",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='co2_emission', y='food_category', data=food)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592c0396-c47e-42d0-ab92-54dcade38e70",
   "metadata": {},
   "source": [
    "> The **beef** `food_category` has clearly the largest IQR (difference between 3rd and 1st quartile). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a7f9f2-6fe9-4707-a734-7dc1d9b010f9",
   "metadata": {},
   "source": [
    "3. Looking at the chart from the previous question, which is the `food_category` with the highest median `co2_emission` value? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb8938-d78a-4f39-beec-515bf22803bc",
   "metadata": {},
   "source": [
    "> The **beef** `food_category` has clearly the highest median value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd7696-498d-4220-89ff-2154275d2469",
   "metadata": {},
   "source": [
    "4. Consider the `consumption` of \"poultry\" and \"fish\" across all available `countries`; looking at the table from question 1, the average of poultry consumption (21.22) seems to be higher than that of fish consumption (17.29), but is this difference statistically significant? Create a permutation test in order to assess the null hypothesis that there is no difference between the two means. **Do you accept or reject the null hypothesis?** Explain why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500bf83f-cdeb-4f80-9ca7-f7bd08172787",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1 = 'poultry'\n",
    "cat2 = 'fish'\n",
    "\n",
    "food_sub = food[food.food_category==cat1][['country','consumption']].merge(food[food.food_category==cat2][['country','consumption']], on='country', how='inner')\n",
    "food_sub.columns = ['country', cat1, cat2]\n",
    "food_sub.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5110c327-78ba-4350-97aa-265aa776073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1_m = food_sub.loc[:,cat1].mean()\n",
    "cat2_m = food_sub.loc[:,cat2].mean()\n",
    "print(f'Avg. {cat1} consumption: {round(cat1_m,2)} \\nAvg. {cat2} consumption: {round(cat2_m,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af42b8-6968-4892-8c0b-98af70fd2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_obs = cat1_m-cat2_m\n",
    "print(f'Avg. Observed Difference: {round(diff_obs,4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18fa26b-45de-4ba9-8a24-ef54eb1a224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "diff_sim = []\n",
    "for sim in range(n): \n",
    "    combined = pd.concat([food_sub.loc[:,cat1], food_sub.loc[:,cat2]])   # combine the two DF columns\n",
    "    np.random.shuffle(combined.values)   # shuffle vlaues in place\n",
    "    rndm1 = combined[:int(len(combined)/2)]   # create first half of randomly chosen elements\n",
    "    rndm2 = combined[int(len(combined)/2):]   # create second half of randomly chosen elements\n",
    "    diff_m = np.mean(rndm1) - np.mean(rndm2)\n",
    "    diff_sim.append(diff_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a12664-b91f-4b30-9513-419d5f25e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(diff_sim)\n",
    "plt.axvline(diff_obs, 0, 1, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f12044-466d-480d-bc6b-8f1c13a33104",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_obs = sum([el > diff_obs for el in diff_sim])\n",
    "p_value = sum([el > diff_obs for el in diff_sim])/len(diff_sim)*100\n",
    "print(f'Nr. of simulated instances more extreme than observed difference: {extreme_obs} out of {len(diff_sim)}')\n",
    "print(f'p-value = {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5801ad9-455f-4bbb-95d6-fa177096ed08",
   "metadata": {},
   "source": [
    "> The p-value is lower than our chosen significance level of 0.05, which means that there isn't enough evidence to sustain the null hypothesis that there is no difference between the two means. Therefore we **reject the null hypothesis** and conclude that **the difference between the two means is statistically significant**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba56ad5-8304-4744-94ed-620fad6c23a5",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "The `world_happiness.csv` dataset from the `data` folder contains a series of variables that can be used as a proxy to a country's evaluation of its own goodness of life. The `happiness_score` metric tries to summarise how \"happy\" each country is. Load and save the dataset to a DataFrame object named `happy`. As always, familiarise yourself with its contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec7d22-d07b-417a-af30-48bc778b4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy = pd.read_csv('data/world_happiness.csv')\n",
    "happy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a9bb6-9d7a-412b-8d19-a4adbf7c63c4",
   "metadata": {},
   "source": [
    "5. Using a histogram, plot the distribution of the `happiness_score` variable, **which distribution does it resemble?** *(in the answer sheet write the [name of the distribution](https://miro.medium.com/max/962/1*DmPUIjvecL7KllOamoFSDw.png) that best fits the data)* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940aded9-7e20-4ce1-beec-07d77d60dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(happy.happiness_score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e1ad5-49f2-4eab-98a6-91967612953c",
   "metadata": {},
   "source": [
    "> The data looks distributed like a Uniform distribution. We can check it using the [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9add8ff-c919-4058-b0b9-ecb574cd0735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stat, p_value = stats.kstest(happy.happiness_score, stats.uniform(loc=0.0, scale=160.0).cdf)\n",
    "print(f'With a p-value={round(p_value,4)}, we accept the null hypothesis that the data is Uniformly distributed. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf499487-473b-48f6-84d4-2df98f1cda1f",
   "metadata": {},
   "source": [
    "6. Plot a correlation matrix (or a correlation heatmap) between all the numeric variables in the dataset. **Which variable is the least correlated with the `happiness_score` metric?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a90ec-cb34-415e-bc0f-c6290aa6b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(happy.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3135c-4d6d-4ec8-8860-af294d83fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy.corr()[np.abs(happy.corr()['happiness_score'])==np.min(np.abs(happy.corr()['happiness_score']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f675a8-16ac-4496-93cd-88874dc90161",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_corr_metric = happy.corr()[np.abs(happy.corr()['happiness_score'])==np.min(np.abs(happy.corr()['happiness_score']))].index[0]\n",
    "print(f'The {min_corr_metric} metric is the least correlated with the happiness_score variable.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2c9c6-ab07-4da9-97ef-71299e1a6bc7",
   "metadata": {},
   "source": [
    "> The corruption metric is the least correlated with the happiness score variable (remember that a correlation of -0.82 represents a strong inverse relation, weak correlations are represented by values close to zero in absolute terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a4db8-5d37-418e-a795-1d23de00c3c6",
   "metadata": {},
   "source": [
    "7. You may have noticed that the `corruption` metric has some missing values. **How many countries have a missing corruption value?** After you've answered the question, replace all missing values *in all columns of the DataFrame* with the *respective column's mean value*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8d6ae-1ccd-47a5-a7d7-3c835e02f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "happy[happy.corruption.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e1466-1250-4602-b1d8-c17cfb0a8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {happy[happy.corruption.isna()].shape[0]} countries with a missing value for the corruption variable. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad6abd6-c557-4b32-b932-8b05e7a5a0b9",
   "metadata": {},
   "source": [
    "> There are 8 countries with a missing value for the corruption variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34bb1aa-a01a-4a3c-b100-a829fd328bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of variables with missing values\n",
    "cols_with_nan = []\n",
    "for col in happy.columns: \n",
    "    if happy[happy[col].isna()].shape[0] > 0: \n",
    "        cols_with_nan.append(col)\n",
    "cols_with_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4fa63f-077a-4537-8637-739f07ebbcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll replace each missing value with the respective variable's mean\n",
    "happy[cols_with_nan].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a5df2-77b6-429b-93ef-7cb8e58da97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaNs with column's mean value\n",
    "happy = happy.fillna(value=happy[cols_with_nan].mean())\n",
    "# checking that there are no more missing data in the DataFrame\n",
    "happy[happy.corruption.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a6d5a1-6adf-4f66-bd9d-911a0e07c85a",
   "metadata": {},
   "source": [
    "8. Use the `statsmodel` package to create a linear regression model where you use `life_exp` to predict the `happiness_score`. After [refreshing your memory on how to interpret a regression coefficient](https://statisticsbyjim.com/regression/interpret-coefficients-p-values-regression/#:~:text=The%20coefficient%20value%20signifies%20how,in%20isolation%20from%20the%20others.), answer the following question: given the model you just created, **a 1-year increase in life expectancy corresponds to an increase of how many points of the happiness score variable?**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00c25e-0479-4fac-93e0-c1a6a87411bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "Y = happy['happiness_score']\n",
    "X = happy[['life_exp']]\n",
    "X = sm.add_constant(data=X)   # add a constant to the model\n",
    "model = sm.OLS(endog=Y,exog=X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455bcc11-5468-48a6-b395-97b1fd4204ba",
   "metadata": {},
   "source": [
    "> Given the model above, an increase of 1-year in life expectancy corresponds to an increase of 5.1 points in the happiness score metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c567dad-9583-4731-aafb-2e887d461403",
   "metadata": {},
   "source": [
    "9. Split the dataset in `train` and `test` sets, leaving 33% of the data in the latter. Then, using the `sklearn` package, train a linear regression model where you try to predict the `happiness_score` using the following set of predictors: `['social_support', 'freedom', 'generosity', 'life_exp']` *(use a `random_state=42`)*. Calculate the R-squared on the train set and compare it with the R-squared based on the test set. **Would you say that the model is overfitting the training data?** Motivate your answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99550bb-a066-4864-a86e-592132711566",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = happy['happiness_score']\n",
    "X = happy[['social_support', 'freedom', 'generosity', 'life_exp']]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef49fea5-49ec-4d25-823b-d797369957e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_squared_train = reg.score(X_train, y_train)\n",
    "r_squared_test = reg.score(X_test, y_test)\n",
    "print(f'The R-squared of the model on the training set is {round(r_squared_train,4)}')\n",
    "print(f'The R-squared of the model on the testing set is {round(r_squared_test,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ceea6-4f08-4d8d-a6bf-df149362b42f",
   "metadata": {},
   "source": [
    "> No, the model doesn't seem to be overfitting the data, since its performance on the training and testing sets is very similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb419a-e0ae-4805-aec4-34523c4e7276",
   "metadata": {},
   "source": [
    "10. Using the same model from the previous question, **calculate the MAE on the testing set and report it on the answers Sheet.** Are you satisfyied with the model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf32829-4281-4d52-96e4-5027f8c3d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = reg.predict(X_test)\n",
    "mae = np.mean(np.abs(y_test - y_hat))\n",
    "print(f'The MAE on the test set is: {round(mae,2)}')\n",
    "mape = np.mean(np.abs((y_test - y_hat)/y_test))*100\n",
    "print(f'The MAPE on the test set is: {round(mape,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f804d-f739-4066-af87-f67e09973f47",
   "metadata": {},
   "source": [
    "> The model's performance could definitely be improved, I wouldn't trust the model to give accurate results since its Mean Absolute Percentage Error is almost 40%. Given the issues of multicollinearity I would probably want to try a different model that doesn't suffer from this problem or I'd try to work on the features, maybe searching for different and more predictive regressors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154416b0-47d5-4420-afe7-989bffebe7b6",
   "metadata": {},
   "source": [
    "### Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a789b3c5-ffcf-4769-a5c9-0487ada0ee3c",
   "metadata": {},
   "source": [
    "11. <span style=\"color:red\">[BONUS]</span> **Do you notice anything strange when looking at the regression coefficients of the model in the previous answer?** (answer here, not on the response Google Sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94f5c92-7abb-48c3-ba1b-a1ae6be7c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = pd.concat([pd.Series(X_test.columns), pd.Series(reg.coef_)], axis=1)\n",
    "coeff.columns = ['predictor', 'coefficient']\n",
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4f3a6-87ee-403f-8e01-8974fc2e434a",
   "metadata": {},
   "source": [
    "> The sign of the first three predictors (`social_support`, `freedom` and `generosity`) are all negative, which means that the more a country is socially supportive, free or generous, the less happy it is which, logically, doesn't make much sense. This should be a fisrt red flag to make you go back to your model and check for statistical significance of the predictors as well as possible traces of multicollinearity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d5a31-b3c4-4696-abd8-6638efd7f5e9",
   "metadata": {},
   "source": [
    "12. <span style=\"color:red\">[BONUS]</span> The code in the following cell creates a forecasting model using the `prophet` library. Specifically, it fits an additive model (the effect of the seasonality is added to the trend in order to get forecasts) on a dataframe `df` which contains the number of airline passengers over time. Notice how the seasonality in the forecast is too large at the start of the time series and too small at the end (compared to the data it tries to fit). **Modify the Prophet code to account for the effect of growing seasonality.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d471a1f-6442-4c8f-a8ea-a94f3258a584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON NOT DELETE >>> RUN THIS CELL!\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "df = pd.read_csv('data/air-passengers.csv')\n",
    "m = Prophet()\n",
    "m.fit(df)\n",
    "future = m.make_future_dataframe(24, freq='MS')\n",
    "forecast = m.predict(future)\n",
    "fig = m.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd60b67-0fcb-44d6-9edc-724f3050dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet(seasonality_mode='multiplicative')\n",
    "m.fit(df)\n",
    "future = m.make_future_dataframe(24, freq='MS')\n",
    "forecast = m.predict(future)\n",
    "fig = m.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e14f6-9633-4a7d-944d-c080138c0f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
